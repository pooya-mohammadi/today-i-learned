# Transformers
## MultiHead Attention:
MultiHead attention is equvalent to filters in CNNs. The best explanation I ever heard.</br>
As we need various filters in CNNs to capture various features, we need different MultiHead attentions to capture different features from input texts or images
