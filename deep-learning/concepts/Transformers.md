# Transformers
## MultiHead Attention:
MultiHead attention is equvalent to filters in CNNs. The best explanation I ever heard.</br>
As we need various filters in CNNs to capture various features, we need different MultiHead attentions to capture different features from input texts or images

![Screenshot from 2023-07-17 19-43-04](https://github.com/pooya-mohammadi/today-i-learned/assets/55460936/69721ab5-99c8-4b71-a849-faa103a61210)

# reference
1. UCF lectures: https://www.youtube.com/watch?v=18ge5ilLZlo&list=PLd3hlSJsX_In7qup928HaHmilugBGctuF&index=2

